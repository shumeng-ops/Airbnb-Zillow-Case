{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airbnb & Zillow Data Challenge \n",
    "### Shumeng Shi -  2021/09\n",
    "\n",
    "- Introduction <br>\n",
    "- Assumptions <br>\n",
    "- Define Functions <br>\n",
    "- Data Preparation <br>\n",
    "- Data Transformation <br>\n",
    "- Visualization <br>\n",
    "- Summary <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Increase cell width for Jupyter\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Introduction\n",
    "A real estate company wants to purchase two-bedroom properties in New York City to rent out short-term as part of their business model. The goal of the project is to figure out the most profitable zip code to invest in.\n",
    "The available data are the estimated Zillow cost for the purchase price and the rent prices of properties on Airbnb. \n",
    "\n",
    "### Step 2: Assumptions\n",
    "\n",
    "1. The time value of money discount rate is 0%.\n",
    "2. All properties and all square feet within each locale can be assumed to be homogeneous.\n",
    "3. Occupancy rate is 75%.\n",
    "4. Cleaning fee is also part of the revenue. Since all the properties need to provide cleaning service between guests checking out and new guests are checking in. Cleaning cost is considered the sunk cost that every host needs to bear. Thus if the host charges an extra cleaning fee, that will be counted as part of the total revenue. This project assumes the average renting nights are 6, and the house gets cleaned every six days. Thus the average cleaning revenue on each day is cleaning fee / 6.\n",
    "5. Import metrics: breakeven year and ROI\n",
    "    - Breakeven Year: the number of years it takes for the company's revenues and expenses become equal\n",
    "    - ROI: Return on investment is a performance measure used to evaluate efficiency or profitability. In this project, we use the percentage of revenue of property value over 25 years as ROI\n",
    "\n",
    "### Step 3: Define Reusable Functions\n",
    "\n",
    "To increase the efficiency and reusability of the code, first define multiple helpful functions for further use. \n",
    "\n",
    "**Function 1: clean_zip(df,zipcode)** <br>\n",
    "clean_zip function takes a dataframe and the name of the zipcode column as input. The functions drop rows with missing zipcode, convert the zipcode to string and keep the first five digits.\n",
    "In this case, only a few zip codes are missing, and it is hard to impute them based on other available information accurately. Dropping those can ensure data accuracy and avoid inviting extra errors.\n",
    "\n",
    "**Function 2: clean_price(df,df, clean_col)** <br>\n",
    "The clean_price function takes a dataframe and a list of columns to be cleaned as input. For each column in the list, the function trims unnecessary symbols ($ or space) from the two ends of the string and converts them to int.\n",
    "\n",
    "\n",
    "**Function 3: profit_analysis(df, value, total_price, occupancy, years)** <br>\n",
    "The profit_analysis function will take a dataframe and several parameters as input. Value is the estimation of property value from Zillow, total_price is the addition of daily rent and cleaning fee. Years is the ROI of specific years.\n",
    "\n",
    "\n",
    "**Function 4: twinx_bar(x_label,y1,y2,xticks, w,label1, label2)** <br>\n",
    "The twinx_bar function creates dual axes histogram plots side by sid. The input parameters are the x label, the value of two y variables(y1,y2), the labels of two y variables(label1,label2). w is the width of the histogram, and xticks is the number of xtickes to be included in the plot.\n",
    "\n",
    "**Function 5: zip_map(width,height,center,df_map,color_category)** <br>\n",
    "The zip_map function creates a dynamic map that marks the location based on zipcode.The parameters are: width and hieght of the figure size. The longitude and latitude of the map center point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shumeng\\Anaconda3\\lib\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "## Import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import folium\n",
    "from branca.element import Figure\n",
    "from geopy.geocoders import Nominatim\n",
    "pd.set_option('display.float_format', lambda x: '%.0f' % x)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_zip(df,zipcode):\n",
    "    ## get rid of the dash in the zipcode\n",
    "    df = df.dropna(subset=['zipcode'])\n",
    "    ## Convert the zipcode to string and get the first 5 digits\n",
    "    df['zipcode'] = df['zipcode'].apply(lambda x: str(x)[:5])\n",
    "    return df\n",
    "\n",
    "def clean_price(df, clean_col):\n",
    "    for col in clean_col:\n",
    "        df[col] = df[col].apply(lambda x: x.lstrip('$').rstrip(' ').replace(',', ''))   ## remove special symbols $ and spaces\n",
    "        df[col] = df[col].apply(lambda x: int(float(x)))   ## convert it to INT\n",
    "    return df\n",
    "\n",
    "def profit_analysis(df, value, total_price, occupancy, years):\n",
    "    ## how many renting days it takes to breakeven\n",
    "    df['breakeven_days'] = df[value] /df[total_price] \n",
    "    ## how many years it takes to breakeven given the occupancy rate\n",
    "    df['breakeven_years'] = df['breakeven_days'] / occupancy/365\n",
    "    ## ROI - the total profit among certain years (revenue - cost) / cost\n",
    "    df['roi'] = (df[total_price]*365*years*occupancy - df[value]) / df[value]\n",
    "    return df\n",
    "\n",
    "def twinx_bar(x_label,y1,y2,xticks, w,label1, label2):\n",
    "    fig,ax1 = plt.subplots(figsize=(15, 8))    ## define a figure and the first axis\n",
    "    bar1 = range(xticks)      ## the x coordinator of first bar    \n",
    "    bar2 = [i+w for i in bar1]   ## the x coordinator of second bar - is w away from the first bar\n",
    "    bar_label = [i+w/2 for i in bar1]    ## the x coordinator of x axis lable - in the middle of the two bars\n",
    "    ax1.bar(bar1,y1,color = \"#eecbff\",width = 0.4,label = label1)    ## draw the first bar chart\n",
    "    ax1.grid(False)    ## hide the grid\n",
    "    ax1.set_ylabel('Breakeven Years')\n",
    "    ax2 = ax1.twinx() ## create second y axis\n",
    "    ax2.bar(bar2,y2,color = \"#95f2c7\",width = 0.4, label = label2)    ## draw the second bar chart\n",
    "    ax2.grid(False)\n",
    "    ax2.set_ylabel('ROI')\n",
    "    plt.xticks(bar_label,x_label)\n",
    "    fig.legend(loc=\"upper right\")\n",
    "\n",
    "    \n",
    "def zip_map(width,height,center,df_map,color_category):\n",
    "    geolocator = Nominatim(user_agent=\"shumeng.shi9@gmail.com\")\n",
    "    for z in range(len(df_map)):\n",
    "        result = geolocator.geocode({\"postalcode\": int(df_map.loc[z,'zipcode'])})   ## for each zipcode in the df, use geolocator to get the center coordinate of that area\n",
    "        df_map.loc[z,'latitude'] = result[1][0]    ## store the latitude in df\n",
    "        df_map.loc[z,'longitude'] = result[1][1]   ## store the longitude in df\n",
    "    \n",
    "    fig = Figure(width=width, height=height)\n",
    "    m=folium.Map(width=width,height=height,location=center, tiles = 'OpenStreetMap',zoom_start=10, min_zoom = 8, max_zoom = 14)  ## create map\n",
    "    for i, row in df_map.iterrows():     ## for each point in the df, mark them on the map with their longitude and latitude\n",
    "        lat = df_map.loc[i,'latitude']\n",
    "        lng = df_map.loc[i,'longitude']\n",
    "        breakeven_years = int(df_map.loc[i,'breakeven_years'])   ## add the zipcode & neighborhood & breakeven_year & ROI onto the popup\n",
    "        neighbourhood = df_map.loc[i,'neighbourhood_group_cleansed']\n",
    "        ROI = round(df_map.loc[i,'roi']*100,2)\n",
    "        popup = df_map.loc[i,'neighbourhood_group_cleansed'] + '<br>' +'zipcode:' +str(df_map.loc[i,'zipcode']) + '<br>' +\"breakeven_years:\" + str(breakeven_years) + 'yrs'+ '<br>'  + \"ROI: \" + str(ROI) +\"%\"\n",
    "        if neighbourhood == color_category[0]:  ## specify different colors to different district\n",
    "            color = 'blue'\n",
    "        elif neighbourhood == color_category[1]:\n",
    "            color = 'red'\n",
    "        else:\n",
    "            color = 'orange'\n",
    "\n",
    "        folium.Marker(location = [lat,lng], popup =popup, icon = folium.Icon(color=color)).add_to(m)\n",
    "    fig.add_child(m)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Data Preparation\n",
    "\n",
    "#### 4.1 Import the data and combine listing data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'Zip_Zhvi_2bedroom.csv' does not exist: b'Zip_Zhvi_2bedroom.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-38ca45463be0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## read the data from local csv file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mz_price\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Zip_Zhvi_2bedroom.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlist_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'listings_file_1_of_4.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlist_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'listings_file_2_of_4.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlist_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'listings_file_3_of_4.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'Zip_Zhvi_2bedroom.csv' does not exist: b'Zip_Zhvi_2bedroom.csv'"
     ]
    }
   ],
   "source": [
    "## read the data from local csv file\n",
    "z_price = pd.read_csv('Zip_Zhvi_2bedroom.csv')\n",
    "list_1 = pd.read_csv('listings_file_1_of_4.csv')\n",
    "list_2 = pd.read_csv('listings_file_2_of_4.csv', header = None)\n",
    "list_3 = pd.read_csv('listings_file_3_of_4.csv', header = None)\n",
    "list_4 = pd.read_csv('listings_file_4_of_4.csv', header = None)\n",
    "\n",
    "## change the column name \n",
    "list_2.columns = list(list_1.columns)\n",
    "list_3.columns = list(list_1.columns)\n",
    "list_4.columns = list(list_1.columns)\n",
    "\n",
    "## combine all the listings file together as big table\n",
    "listing = pd.concat([list_1, list_2,list_3,list_4],axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Select Relevant Data\n",
    "1. Zillow Price:\n",
    "    - Rows Selection: The analysis confines two-bedroom properties in NYC. Limit data entry to only in NYC. Because the city column in Zillow df is messy and many are incorrectly tagged, we decided to use the state column and metro to limit NYC entries.\n",
    "    - Columns selection: The property price from many years ago can't accurately reflect its value today. Also, there are many missing values in those columns. We only keep the most recent years' cost as a reference.\n",
    "2. Airbnb listing:\n",
    "    - Rows Selection:For the Airbnb listing data, New York comprises five areas: Brooklyn, Queens, Manhattan, the Bronx. Select those areas in the Airbnb data with two bedrooms. \n",
    "    - Columns selection:There are many host and facilities information related to the specific property in the Airbnb data. Those don't bring value in analyzing the profitability by zipcode. Thus, we only keep the location-related information for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter out to values only for NYC\n",
    "z_price = z_price[(z_price['State'] == 'NY') & (z_price['Metro'] == 'New York')].reset_index()\n",
    "listing = listing[(listing['state'].str.contains('NY','New York')) & (listing['city'].str.lower().str.contains(\"new york|brooklyn|queens|manhattan|bronx\")) & (listing['bedrooms'] == 2)].reset_index()\n",
    "\n",
    "\n",
    "## select only related features，any host related features and facility amendity features are not relavent, because we only care about the property itself, not the add -on values\n",
    "listing = listing[['street','neighbourhood_group_cleansed','city','state','zipcode','latitude','longitude','square_feet','price','cleaning_fee']]\n",
    "\n",
    "## for the zillow data set, we choose the latest price level for analysis\n",
    "z_price = z_price[['RegionName','SizeRank','2017-01','2017-02','2017-03','2017-04','2017-05','2017-06']]\n",
    "z_price['avg_price'] = z_price[['2017-01','2017-02','2017-03','2017-04','2017-05','2017-06']].mean(axis=1)\n",
    "\n",
    "print(\"The shape of zillow table \" + str(z_price.shape))\n",
    "print(\"The shape of listing table \" + str(listing.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data Cleaning  - Perform outlier detection, null value imputation, data cleanup to ensure data quality\n",
    "##### 4.3.1 Zillow price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check null value and duplicated rows\n",
    "print(z_price.isnull().sum()) \n",
    "print(\"There are \" + str(sum(z_price.duplicated(['RegionName']))) +\" duplicated zipcode in the table\")\n",
    "print(\"There are \" + str(sum(z_price.duplicated())) +\" duplicated row in the table\")\n",
    "\n",
    "## change column name from RegionName to zipcode\n",
    "z_price = z_price.rename(columns={\"RegionName\": \"zipcode\"})\n",
    "## use the cusomized function \"clean_zip\" to clean the zipcode in the z_price df\n",
    "z_price = clean_zip(z_price,'zipcode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no null value and no duplicatd rows in the zillow dataset. Next, examine the distribution of the price column by ploting the histogram of prices in 2017-06 and the average prices in the past 6 months side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the distribution of price on 201706\n",
    "## even though some of the house have very large value, but I checked the values in the past few months, they are pretty steady, so they are not outlier\n",
    "fig, ax = plt.subplots(1,2,figsize=(18, 6))\n",
    "#plt.figure(figsize=(8, 6), dpi=80)\n",
    "prices_x = z_price['2017-06']\n",
    "prices_x_2 = z_price['avg_price']\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "bins = [0,500000, 1000000, 1500000,2000000,2500000,3000000,3500000]\n",
    "# prices_x.hist(bins = bins, ax = axes[0],edgecolor = 'black')\n",
    "# prices_x_2.hist(bins = bins, ax = axes[1],edgecolor = 'black')\n",
    "# plt.show()\n",
    "\n",
    "ax[0].hist(prices_x, bins, alpha = 0.5, color = 'r', label = '2017-06 price')\n",
    "\n",
    "ax[1].hist(prices_x_2, bins, alpha = 0.5, color = 'g', label = '6 month average price')\n",
    "ax[0].set_title('House Price Histogram - 2017-06')\n",
    "ax[1].set_title('House Price Histogram - 6 Month Average')\n",
    "##fig.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "z_price = z_price[['zipcode','SizeRank','2017-06']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After plotting the distribution of house prices on 2017-06, and the price distribution of the six-month average (2017-01 to 2017-06), there is no significant difference between these two. We can conclude that even if very few properties have extremely high prices, that is likely not an error but a fair value. Also, we will use the last month as the final analysis price due to no significant difference between those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2 Airbnb data - Drop out square_feet and clean up price, cleaning_fee and zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the % of missing value\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "listing.isnull().sum() / len(listing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "98% of the square feet is missing, and it does not bring us valuable information. For analysis purposes, delete the column. About 20% of the cleaning fee is empty, impute the empty value with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## because 98% of the square_feet is null, delete the column \n",
    "listing = listing.drop(['square_feet'], axis = 1)\n",
    "listing['cleaning_fee'] = listing['cleaning_fee'].fillna('0')\n",
    "## Clean the price, cleaning_fee and zipcode with customized function\n",
    "listing = clean_price(listing, ['price','cleaning_fee'])\n",
    "listing = clean_zip(listing,'zipcode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Data Transformation -- Join Zillow data and Airbnb data, conduct profitability analysis and select zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(left=listing, right=z_price, left_on='zipcode', right_on='zipcode')\n",
    "## We assume the apartment is cleaned every six days, the daily revenue of cleaning fee is 1/6 of the total cleaning_fee\n",
    "df['total_price'] = df['price'] + df['cleaning_fee'] / 6\n",
    "## rename the value column\n",
    "df = df.rename(columns={\"2017-06\": \"value\"})\n",
    "df = df.drop(['street','city','state','price','cleaning_fee','SizeRank','latitude','longitude'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the customized function profit_analysis to get the breakeven years of each property and the ROI of a 25-year investment period, calculate the average breakeven year and ROI of all the properties in that zip code. Select the common nine zip codes that are on the top of both lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = profit_analysis(df,'value','total_price',0.75,25)\n",
    "df_agg = df.groupby(['zipcode','neighbourhood_group_cleansed']).mean()\n",
    "df_agg = df_agg.sort_values(by=['breakeven_years'], ascending=True).reset_index()\n",
    "\n",
    "zip_years = list(df_agg.sort_values(by=['breakeven_years'], ascending=True)[:10]['zipcode'])\n",
    "zip_roi = list(df_agg.sort_values(by=['roi'], ascending=False)[:10]['zipcode'])\n",
    "zip_selected = set(zip_roi) & set(zip_years)\n",
    "zip_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Data Visualization - Plot the breakeven year and ROI to select recommended zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_y = df_agg[df_agg.zipcode.isin (zip_selected)]['breakeven_years']\n",
    "roi_y = df_agg[df_agg.zipcode.isin (zip_selected)]['roi']\n",
    "zip_selected = df_agg[df_agg.zipcode.isin (zip_selected)]['zipcode']\n",
    "## Apply the customized function to make the bar charts\n",
    "twinx_bar(zip_selected,year_y,roi_y,9,0.4,\"breakeven year\",\"ROI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The side-by-side bar chart shows that the first two zip codes - 11234 and 11434 have the smallest breakeven year, about 16 years. And zipcode 1,2,3,7 have relatively high ROI, ranging from 40% to 60%. Based on the above analysis, we decide to recommend those 4 zip codes for purchase - ['11234','11434','10036','10025']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_recommend = ['11234','11434','10036','10025']\n",
    "## filter only those 4 zipcodes to plot on map\n",
    "df_map =df_agg[df_agg.zipcode.isin(zip_recommend)].reset_index(drop = True)\n",
    "## use the customized zip_map function to plot the marks on the map\n",
    "zip_map(650,450,[40.7850,-73.9682],df_map,['Manhattan','Brooklyn','Queens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend 4 zipcodes to invest in based on their breakeven year and higher ROI among 25-year period.\n",
    "- **zipcode 11434 in Queens**: Among the four recommended zipcodes, properties in Queens have the lowest value on Zillow compared to other districts in NYC like Manhattan.  It costs about \\\\$380k, and the average daily rent is \\\\$102. It will take about 17 years to break even and start earning profit. After 25 years of operation, the ROI will be 83%.\n",
    "- **zipcode 11234 in Brooklyn**: Among the four recommend zipcodes, the property cost in Brooklyn is in the middle.  It costs about \\\\$476k, and the average daily rent is \\\\$112. It will take about 16 years to break even and start earning profit. After 25 years of operation, the ROI will be 61%.\n",
    "- **zipcode 10036 in Manhanttan**: Among the four recommended zipcodes, Manhattan's property cost and daily rent are the highest, and it takes longer to break even.  It costs about \\\\$1.7M, and the average daily rent is \\\\$378. It will take about 23 years to break even and start earning profit. After 25 years of operation, the ROI will be 51%. But once it breaks even, the profit growth speed is higher due to the higher daily rent.\n",
    "- **zipcode 10036 in Manhanttan**: Among the four recommend zipcodes, the property cost in Manhanttan and daily rent are the highest, and it takes longer to break even.  It costs about \\\\$1.7M, and the average daily rent is \\\\$297. It will take about 24 years to break even and start earning profit. After 25 years of operation, the ROI will be 42%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Future Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Missing zip code in the listing table**: Due to time limitations, we drop the records with missing zip codes in the analysis. With more time, it would be better to leverage $geopy.geocoders$ library to get the valid zip code from longitude and latitude. That will give us more information and help make a better decision\n",
    "- **Property cost**: In the analysis, we use the Zillow cost from 2017/06 as the cost for analysis. Suppose the analysis is conducted to help make a decision today in 2021. In that case, we can either collect more recent dates or run a time series model to predict the property cost to reflect the situation better.\n",
    "- **Discount rate**: In this case, we use the daily rent in the calculation. If time permits, we could better estimate the average renting period in different districts or zipcodes. Then we could use the weekly rent and monthly rent to predict revenue and profit better. \n",
    "- **Model in predicting occupancy rate**: Instead of using the universal 75% across all properties, we could run a machine learning model with property features(amenities, location), host information, and renter reviews to get a customized occupancy rate for different districts\n",
    "- **More metrics to evaluate recommended zipcode**: If time permits, we can add more metrics in our analysis, like the annual revenue, annual profit, ROI in 10 years, 20 years, 30 years, etc. Also, instead of getting the average of those metrics across all properties from a specific zip code, we could calculate each zipcode's median or mode to complement our analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
